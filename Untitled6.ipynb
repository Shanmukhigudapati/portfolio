{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDzu+Dt9Sd7wbdnAiqowgA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shanmukhigudapati/portfolio/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QiLTqZfzS6c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "783fcd43-809e-4622-c99d-12975034b2fc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c52550a06aed>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_intent_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_slot_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "from utils import get_intent_labels, get_slot_labels\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        words: list. The words of the sequence.\n",
        "        intent_label: (Optional) string. The intent label of the example.\n",
        "        slot_labels: (Optional) list. The slot labels of the example.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, guid, words, intent_label=None, slot_labels=None):\n",
        "        self.guid = guid\n",
        "        self.words = words\n",
        "        self.intent_label = intent_label\n",
        "        self.slot_labels = slot_labels\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, intent_label_id, slot_labels_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.intent_label_id = intent_label_id\n",
        "        self.slot_labels_ids = slot_labels_ids\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class JointProcessor(object):\n",
        "    \"\"\"Processor for the JointBERT data set \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.intent_labels = get_intent_labels(args)\n",
        "        self.slot_labels = get_slot_labels(args)\n",
        "\n",
        "        self.input_text_file = 'seq.in'\n",
        "        self.intent_label_file = 'label'\n",
        "        self.slot_labels_file = 'seq.out'\n",
        "\n",
        "    @classmethod\n",
        "    def _read_file(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = []\n",
        "            for line in f:\n",
        "                lines.append(line.strip())\n",
        "            return lines\n",
        "\n",
        "    def _create_examples(self, texts, intents, slots, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for i, (text, intent, slot) in enumerate(zip(texts, intents, slots)):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            # 1. input_text\n",
        "            words = text.split()  # Some are spaced twice\n",
        "            # 2. intent\n",
        "            intent_label = self.intent_labels.index(intent) if intent in self.intent_labels else self.intent_labels.index(\"UNK\")\n",
        "            # 3. slot\n",
        "            slot_labels = []\n",
        "            for s in slot.split():\n",
        "                slot_labels.append(self.slot_labels.index(s) if s in self.slot_labels else self.slot_labels.index(\"UNK\"))\n",
        "\n",
        "            assert len(words) == len(slot_labels)\n",
        "            examples.append(InputExample(guid=guid, words=words, intent_label=intent_label, slot_labels=slot_labels))\n",
        "        return examples\n",
        "\n",
        "    def get_examples(self, mode):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode: train, dev, test\n",
        "        \"\"\"\n",
        "        data_path = os.path.join(self.args.data_dir, self.args.task, mode)\n",
        "        logger.info(\"LOOKING AT {}\".format(data_path))\n",
        "        return self._create_examples(texts=self._read_file(os.path.join(data_path, self.input_text_file)),\n",
        "                                     intents=self._read_file(os.path.join(data_path, self.intent_label_file)),\n",
        "                                     slots=self._read_file(os.path.join(data_path, self.slot_labels_file)),\n",
        "                                     set_type=mode)\n",
        "\n",
        "\n",
        "processors = {\n",
        "    \"atis\": JointProcessor,\n",
        "    \"snips\": JointProcessor\n",
        "}\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, max_seq_len, tokenizer,\n",
        "                                 pad_token_label_id=-100,\n",
        "                                 cls_token_segment_id=0,\n",
        "                                 pad_token_segment_id=0,\n",
        "                                 sequence_a_segment_id=0,\n",
        "                                 mask_padding_with_zero=True):\n",
        "    # Setting based on the current model type\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    unk_token = tokenizer.unk_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 5000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "        # Tokenize word by word (for NER)\n",
        "        tokens = []\n",
        "        slot_labels_ids = []\n",
        "        for word, slot_label in zip(example.words, example.slot_labels):\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "            if not word_tokens:\n",
        "                word_tokens = [unk_token]  # For handling the bad-encoded word\n",
        "            tokens.extend(word_tokens)\n",
        "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
        "            slot_labels_ids.extend([int(slot_label)] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        # Account for [CLS] and [SEP]\n",
        "        special_tokens_count = 2\n",
        "        if len(tokens) > max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
        "            slot_labels_ids = slot_labels_ids[:(max_seq_len - special_tokens_count)]\n",
        "\n",
        "        # Add [SEP] token\n",
        "        tokens += [sep_token]\n",
        "        slot_labels_ids += [pad_token_label_id]\n",
        "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        # Add [CLS] token\n",
        "        tokens = [cls_token] + tokens\n",
        "        slot_labels_ids = [pad_token_label_id] + slot_labels_ids\n",
        "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = max_seq_len - len(input_ids)\n",
        "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
        "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "        slot_labels_ids = slot_labels_ids + ([pad_token_label_id] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
        "        assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_ids), max_seq_len)\n",
        "        assert len(slot_labels_ids) == max_seq_len, \"Error with slot labels length {} vs {}\".format(len(slot_labels_ids), max_seq_len)\n",
        "\n",
        "        intent_label_id = int(example.intent_label)\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % example.guid)\n",
        "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
        "            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
        "            logger.info(\"intent_label: %s (id = %d)\" % (example.intent_label, intent_label_id))\n",
        "            logger.info(\"slot_labels: %s\" % \" \".join([str(x) for x in slot_labels_ids]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          token_type_ids=token_type_ids,\n",
        "                          intent_label_id=intent_label_id,\n",
        "                          slot_labels_ids=slot_labels_ids\n",
        "                          ))\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, mode):\n",
        "    processor = processors[args.task](args)\n",
        "\n",
        "    # Load data features from cache or dataset file\n",
        "    cached_features_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        'cached_{}_{}_{}_{}'.format(\n",
        "            mode,\n",
        "            args.task,\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            args.max_seq_len\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        # Load data features from dataset file\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        if mode == \"train\":\n",
        "            examples = processor.get_examples(\"train\")\n",
        "        elif mode == \"dev\":\n",
        "            examples = processor.get_examples(\"dev\")\n",
        "        elif mode == \"test\":\n",
        "            examples = processor.get_examples(\"test\")\n",
        "        else:\n",
        "            raise Exception(\"For mode, Only train, dev, test is available\")\n",
        "\n",
        "        # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
        "        pad_token_label_id = args.ignore_index\n",
        "        features = convert_examples_to_features(examples, args.max_seq_len, tokenizer,\n",
        "                                                pad_token_label_id=pad_token_label_id)\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    all_intent_label_ids = torch.tensor([f.intent_label_id for f in features], dtype=torch.long)\n",
        "    all_slot_labels_ids = torch.tensor([f.slot_labels_ids for f in features], dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask,\n",
        "                            all_token_type_ids, all_intent_label_ids, all_slot_labels_ids)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8dqVZWQePbaX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}